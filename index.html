<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="files/jemdoc.css" type="text/css" />
<title>Yanzhao Shi-Home Page</title>
</head>
<body>

<div class="menu"> 
    <a href="#home"><b>Home</b></a>
    <a href="#awards"><b>Awards</b></a>
    <a href="#publications"><b>Research</b></a>
    <a href="#services"><b>Services</b></a>
</div>

<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 
    
<div id="toptitle">
<h1>Yanzhao Shi&nbsp;&nbsp;时彦钊</h1>
</div>

<table class="imgtable" width="100%">
    <tr>
        <td align="right">
            <p>
                College of Computer Science, <a href="https://www.bjut.edu.cn/">Beijing University of Technology</a><br/>
                Email: <font face="courier new, monospace">yanzhaoshi0927@outlook.com</font><br/>
                <br/>
                <a href="https://scholar.google.com/citations?user=pRARRDQAAAAJ&hl=zh-CN"><img src="./files/google_scholar.png" height="20px" style="margin-bottom:-3px">&nbsp;Google Scholar</a>&nbsp;
                <a href="https://www.github.com/yanzhaoshi"><img src="./files/github_s.jpg" height="20px" style="margin-bottom:-3px">&nbsp;Github</a>&nbsp;
                <!-- <a href="https://www.researchgate.net/profile/Yuting-He-17?ev=hdr_xprf"><img src="./files/rg.png" height="20px" style="margin-bottom:-3px">&nbsp;ResearchGate</a>&nbsp; -->
                <!-- <a href="https://drive.google.com/file/d/1fCahhpdNe__-Mo5VCY-cIpNDNle4jx0J/view?usp=share_link"><img src="./files/cv.png" height="20px" style="margin-bottom:-3px">&nbsp;Curriculum Vitae</a>&nbsp; -->
            </p>
        </td>
        <td align="left" width="250px">
            <!-- <img src="./files/yanzhaoshi.jpg" alt="" height="250px" /> -->
            <img src="./files/证件照-蓝底.jpg" alt="" height="250px" />
        </td>
    </tr>
</table>
    
<h2>Biography</h2>


Welcome! I am currently in my final year as a Master's student studying Computer Science and Technology at Beijing University of Technology, supervised by Professor <a href="https://xxxb.bjut.edu.cn/info/1409/2196.htm">Junzhong Ji</a>, co-supervised by Associate Professor <a href="https://xxxb.bjut.edu.cn/info/1409/2213.htm">Xiaodan Zhang</a>.
I am also remotely advised by Assistant Professor <a href="https://liangqiong.github.io/">Liangqiong Qu</a> at the University of Hong Kong. 
I deeply appreciate the dedicated guidance and support.
<br/>
My current research topics are medical AI and deep learning.
I mainly focus on medical report generation and medical vision-language pretraining, involving biomedical informatics, computer vision, and natural language processing.
Besides, I'm also interested in contrastive learning, knowledge graph, and large vision language models.
The long-term objective of my research is to explore the most advanced methods for enhancing the explainability, fairness, safety, and human-centric nature of trustworthy medical AI models.

<!-- Drawing upon biomedical informatics, computer vision, and natural language processing, 
are medical report generation and medical vision-language pretraining.
Besides, I mainly focuse on contrastive learning, knowledge graph, curriculum learning, and large vision language models.
The long-term research goal of my study is to explore the most advanced trustworthy medical AI, improving the explainability, fairness, safety, and human-centric. -->
<!-- My interests focuses on Trustworthy Medical AI systems for computer-aided clinical diagnosis.
developing novel methodologies to 
inject useful knowledge into medical AI systems , including knowledge-guided medical report generation systems.
I'm also interested in contrastive learning, knowledge graph, curriculum learning, and large vision language models. -->
<!-- Besides, I'm open and willing to explore other trustworthy medical tasks, e.g. interpretable diagnostic models, unbiased  -->
<!-- Human-Centered Medical AI, Knowledge Graph, Natural Language Generation -->
<!-- My research interests include the combination of Computer Vision, Natural Language Processing, Causality, and Biomedicine. -->
<!-- My current research focus on the fields of <strong>Medical Report Generation</strong> and <strong>Medical Image-Text Representation Learning</strong>, 
which related to both the Computer Vision (CV) and the Natural Language Processing (NLP). 
I am dedicated to advancing knowledge in these areas through rigorous research and innovative solutions. -->
<br><br>
<!-- <a href="https://yanzhaoshi.github.io/research">Here</a> is a list of my published research. -->
<!-- More information is listed in <a href="https://yanzhaoshi.github.io/about">About Me</a>. -->
<!-- <br><br> -->

<!-- <span style="color: blue; font-size: larger;"><strong>I am a self-movited student actively seeking for Ph.D. position starting from 2025 Fall.</strong></span> -->
<span style="color: blue; font-size: larger;">I will join HKU-Qu-Lab for my Ph.D. studies starting from 2025 Fall, focusing on medical multi-modal large language models.</span>

<h2>News</h2>
    <div style="overflow: auto;">
        <ul> 
            <li>[2024.09] One paper was accepted by EMNLP 2024 Findings.</li>
            <li>[2024.08] We proposed a LLM-based gaming AI (Llama_Dou) that achieves first runner-up at the Chinese Collegiate Computer Gaming AI Contest.</li>
            <li>[2024.02] One paper was accepted by Multimedia Systems (JCR Q1).</li>
            <li>[2024.01] One paper was accepted by Multimedia Systems (JCR Q1).</li>
            <li>[2023.11] One paper was accepted by Computers in Biology and Medicine (JCR Q1).</li>
            <li>[2023.10] One paper was accepted to EMNLP 2023 (Oral Presentation) Paper.</li>
            <li>[2023.06] One paper was nominated as the best paper of ChinaMM 2023.</li>
            <li>[2023.05] Two papers were accepted to ChinaMM 2023 Paper.</li>
        </ul>
    </div>
    
<a id="awards" class="anchor"></a>
<h2>Awards and Honors</h2>
    <div>
        <ul>
            <li>2023, Outstanding Master’s Student, Beijing University of Technology</li>
            <li>2022-2023, Master’s Degree First-Class Academic Scholarship (every year)</li>
            <li>2023, Best Paper Award Nomination at ChinaMM 2023</li>
            <li>2023, Chinese College Student Computer Game AI Competition – First Prize</li>
            <li>2022, Outstanding Graduate, Shandong Province</li>
            <li>2022, Outstanding Graduate, University of Jinan</li>
            <li>2018-2022, Undergraduate First-Class Academic Scholarship (every year)</li>
            <li>2018-2022, Outstanding Undergraduate Student, University of Jinan (every year)</li>
        </ul>
    </div>

<a id="publications" class="anchor"></a>
    
<h2>Publications [<a href="https://scholar.google.com/citations?user=pRARRDQAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a>]</h2>
    <h3>☆ Medical Report Generation</h3>
        <table class="imgtable" width="100%" bgcolor="#F0F0F0">
            <tr>
                <td>
                    <p class="pub_title">Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation.</p>
                    <p class="pub_author"><u><b>Yanzhao Shi</b></u>, Junzhong Ji, Xiaodan Zhang, Liangqiong Qu and Ying Liu.<br/>
                    <i>The 2023 Conference on Empirical Methods in Natural Language Processing</i> (<b>Oral Long Paper</b>)<br/>
                    <font color="#922B21"><b>&nbsp;EMNLP 2023&nbsp;</b></font>
                    |<a href= "https://aclanthology.org/2023.emnlp-main.408/">&nbsp;Paper&nbsp;</a>
                    <!-- |<a href="https://github.com/YutingHe-list/GVSL">&nbsp;Code&nbsp;</a> -->
                    <!-- |<a href="./files/poster_GVSL.pdf">&nbsp;Poster&nbsp;</a> -->
                    <!-- |<a href="./files/slide_GVSL.pdf">&nbsp;Slide&nbsp;</a> -->
                    <!-- |<a href="https://x-ark.github.io/">&nbsp;Homepage&nbsp;</a> -->
                    |<a href="./files/bib_PGCA.txt">&nbsp;Bibtex&nbsp;</a>
                    <!-- |<a href="./files/paper_Chinese_GVSL.pdf">&nbsp;中译版&nbsp;</a> -->
                    </p>
                </td>
            </tr>
            
            <tr>
                <td>
                    <p class="pub_title">Weakly Guided Attention Model with Hierarchical Interaction for Brain CT Report Generation.</p>
                    <p class="pub_author">Xiaodan Zhang, Sisi Yang, <u><b>Yanzhao Shi</b></u>, Junzhong Ji, Ying Liu, Zheng Wang and Huimin Xu.<br/>
                    <i>Computers in Biology and Medicine</i><br/>
                    <font color="#922B21"><b>&nbsp;Comput Biol Med 2023&nbsp;</b></font>
                    |<a href= "https://doi.org/10.1016/j.compbiomed.2023.107650">&nbsp;Paper&nbsp;</a>
                    <!-- |<a href="https://github.com/ChenXiaoFei-CS/KoBo">&nbsp;Code&nbsp;</a> -->
                    <!-- |<a href="./files/poster_KoBo.pdf">&nbsp;Poster&nbsp;</a> -->
                    |<a href="./files/bib_WGAM-HI.txt">&nbsp;Bibtex&nbsp;</a>
                    </p>
                </td>
            </tr>
            
            <tr>
                <td>
                    <p class="pub_title">Prior Tissue Knowledge-Driven Contrastive Learning for Brain CT Report Generation.</p>
                    <p class="pub_author"><u><b>Yanzhao Shi</b></u>, Junzhong Ji, Xiaodan Zhang, Ying Liu, Zheng Wang and Huimin Xu.<br/>
                    <i>Multimedia Systems</i><br>
                    <font color="#922B21"><b>&nbsp;Multimedia Syst 2024&nbsp;</b></font>
                    |<a href= "https://doi.org/10.1007/s00530-024-01289-w">&nbsp;Paper&nbsp;</a>
                    |<a href="./files/bib_MRCL.txt">&nbsp;Bibtex&nbsp;</a>
                    </p>
                </td>
            </tr>

            <tr>
                <td>
                    <p class="pub_title">GHCL: Gaussian Heuristic Curriculum Learning for Brain CT Report Generation.</p>
                    <p class="pub_author">Qingya Shen, <u><b>Yanzhao Shi</b></u>, Xiaodan Zhang, Junzhong Ji, Ying Liu and Huimin Xu.<br/>
                    <i>Multimedia Systems</i><br>
                    <font color="#922B21"><b>&nbsp;Multimedia Syst 2024&nbsp;</b></font>
                    |<a href= "https://doi.org/10.1007/s00530-024-01266-3">&nbsp;Paper&nbsp;</a>
                    |<a href="./files/bib_GHCL.txt">&nbsp;Bibtex&nbsp;</a>
                    </p>
                </td>
            </tr>

        </table>
    
<!-- Services -->
<a id="services" class="anchor"></a>
<h2>Services</h2>

    
<p>Reviewer: </p>
<ul>
ACM MM 2024; 

ACL ARR February 2024.
</ul>


</body>
</html>
